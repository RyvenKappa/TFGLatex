@misc{barriossanchezPruebaRedEvaluando2023,
  type = {{info:eu-repo/semantics/bachelorThesis}},
  title = {{La prueba de la red: evaluando su repetibilidad y el efecto del ayuno en la trucha arcoiris (Oncorhynchus mykiss)}},
  shorttitle = {{La prueba de la red}},
  author = {Barrios S{\'a}nchez, Juan Enrique},
  year = {2023},
  month = nov,
  publisher = {E.T.S. de Ingenier{\'i}a Agron{\'o}mica, Alimentaria y de Biosistemas (UPM)},
  address = {Madrid},
  url = {https://oa.upm.es/77142/},
  urldate = {2024-04-04},
  abstract = {El bienestar animal y la optimizaci{\'o}n de procesos productivos se ha convertido en un punto clave en la industria animal debido a la incipiente concienciaci{\'o}n por parte de los consumidores. La importancia de la industria acu{\'i}cola en la producci{\'o}n animal radica, entre otras cosas, en su valor como fuente proteica. Es por ello que el n{\'u}mero de art{\'i}culos cient{\'i}ficos relacionados con la etolog{\'i}a y el bienestar en peces ha aumentado de manera significativa en los {\'u}ltimos a{\~n}os, al igual que en el resto de especies zoot{\'e}cnicas. Sin embargo, la literatura existente acerca de la eficacia de los test de personalidad con el fin de mejorar las caracter{\'i}sticas productivas es escasa. En este Trabajo Fin de Grado se han estudiado diversos aspectos (repetibilidad, valoraci{\'o}n de la personalidad, efecto del ayuno y descanso entre movimientos) de la prueba de la red (net-test), la cual se usa normalmente para diferenciar entre peces proactivos y reactivos. La prueba consiste en extraer y mantener al pez fuera del agua con una red durante un determinado tiempo para cuantificar los movimientos de escape o saltos. Se utilizaron 90 truchas arco{\'i}ris (Oncorhynchus mykiss) con un peso medio de 368 {\textpm} 43,44 g, separadas en nueve grupos (10 peces por tanque). Una vez a la semana durante un mes (28 d{\'i}as) los peces fueron sometidos al test durante 15 segundos, extrayendo a los individuos de manera individual. La catalogaci{\'o}n conductual basada en los movimientos se dividi{\'o} en proactivos, para los peces que realizaban mayor n{\'u}mero de movimientos, reactivos para los que registraban un n{\'u}mero de movimientos inferior a la media, y neutral para los peces que se encontraban en la media. En la tercera semana se aplicaron tres tratamientos de ayuno: tres grupos control de peces sin restricci{\'o}n alimentaria; tres grupos con un ayuno inferior a 60 grados d{\'i}a (4 d{\'i}as y 54ºC d); y otros tres grupos con un ayuno superior a 80 grados d{\'i}a (7 d{\'i}as y 94,5ºC d). Se confirm{\'o} la repetibilidad de la prueba al no existir diferencias significativas a lo largo de los test. Asimismo, la reducci{\'o}n del tiempo de prueba a 10 segundos demostr{\'o} ser un buen modelo para dictaminar la personalidad del pez frente a la reducci{\'o}n de 5 segundos, coincidiendo en que la categor{\'i}a neutra podr{\'i}a afectar a la precisi{\'o}n y sensibilidad. Adem{\'a}s, se realiz{\'o} un PCA, incluyendo las variables de latencia de escape (PC1 = -0,336), tiempo total escapando (PC1 = 0,754) y el n{\'u}mero total de movimientos (PC1 = 0,704), observ{\'a}ndose que los peces con una latencia de escape m{\'a}s peque{\~n}a, presentaron valores m{\'a}s altos en las otras dos variables, siendo estos peces m{\'a}s proactivos. Se observ{\'o} que el ayuno no present{\'o} diferencias significativas en el n{\'u}mero de movimientos, lo que supone que el test se puede validar en cualquier estado de alimentaci{\'o}n. Respecto a las franjas de descanso de los peces dentro de la red, no se apreciaron diferencias significativas al aumentar el tiempo de las pruebas, de forma que no influy{\'o} sobre el estudio de personalidad en peces. En conclusi{\'o}n, se observ{\'o} que la prueba de la red presenta resultados similares a lo largo del tiempo, pudiendo categorizar a los individuos seg{\'u}n su conducta y bajo cualquier r{\'e}gimen alimenticio, recomendando una duraci{\'o}n {\'o}ptima de entre 10 y 15 segundos en futuras investigaciones etol{\'o}gicas.},
  collaborator = {Villarroel Robinson, Morris Ricardo and de la LLave Prop{\'i}n, {\'A}lvaro},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/3.0/es/},
  langid = {spanish}
}

@inproceedings{bhagyaOverviewDeepLearning2019,
  title = {An {{Overview}} of {{Deep Learning Based Object Detection Techniques}}},
  booktitle = {2019 1st {{International Conference}} on {{Innovations}} in {{Information}} and {{Communication Technology}} ({{ICIICT}})},
  author = {Bhagya, C. and Shyna, A.},
  year = {2019},
  month = apr,
  pages = {1--6},
  doi = {10.1109/ICIICT1.2019.8741359},
  url = {https://ieeexplore.ieee.org/document/8741359},
  urldate = {2024-02-14},
  abstract = {Recent years have witnessed a boundless growth in the field of deep learning. With the preferment in the field of deep learning, the task of object detection has become more exciting and challenging. Object detection focuses on detecting the presence of entire objects within a given image. Deep learning based object detection techniques have shown an efficacy to learn the object features directly from the data. The paper mainly focuses on providing a survey on various state-of-the-art deep learning based object detection techniques. The work also concentrates on providing an extensive comparison regarding the opportunities and obstacles faced by different object detection techniques. The paper concludes by identifying the future golden scopes for research in these fields.},
  keywords = {Computer vision,Deep learning,Feature extraction,Microsoft Windows,Object detection,Proposals,Task analysis}
}

@article{chuangTrackingLiveFish2015,
  title = {Tracking {{Live Fish From Low-Contrast}} and {{Low-Frame-Rate Stereo Videos}}},
  author = {Chuang, Meng-Che and Hwang, Jenq-Neng and Williams, Kresimir and Towler, Richard},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {25},
  number = {1},
  pages = {167--179},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2014.2357093},
  url = {https://ieeexplore.ieee.org/document/6898002},
  urldate = {2024-03-06},
  abstract = {Nonextractive fish abundance estimation with the aid of visual analysis has drawn increasing attention. Unstable illumination, ubiquitous noise, and low-frame-rate (LFR) video capturing in the underwater environment, however, make conventional tracking methods unreliable. In this paper, we present a multiple fish-tracking system for low-contrast and LFR stereo videos with the use of a trawl-based underwater camera system. An automatic fish segmentation algorithm overcomes the low-contrast issues by adopting a histogram backprojection approach on double local-thresholded images to ensure an accurate segmentation on the fish shape boundaries. Built upon a reliable feature-based object matching method, a multiple-target tracking algorithm via a modified Viterbi data association is proposed to overcome the poor motion continuity and frequent entrance/exit of fish targets under LFR scenarios. In addition, a computationally efficient block-matching approach performs successful stereo matching that enables an automatic fish-body tail compensation to greatly reduce segmentation error and allows for an accurate fish length measurement. Experimental results show that an effective and reliable tracking performance for multiple live fish with underwater stereo cameras is achieved.},
  keywords = {Cameras,Estimation,Fish abundance estimation,low-frame-rate (LFR) video,Marine animals,multiple target tracking,stereo imaging,Stereo vision,Target tracking,underwater video,Videos}
}

@misc{ContourFeaturesOpenCVPython,
  title = {Contour {{Features}} --- {{OpenCV-Python Tutorials}} Beta Documentation},
  url = {https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.html},
  urldate = {2024-02-22}
}

@misc{CUDAProgrammingGuide,
  title = {{{CUDA C}}++ {{Programming Guide}}},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities},
  urldate = {2024-03-07}
}

@misc{DirectorGeneralFAO,
  title = {{El director general de la FAO destaca el papel esencial de la acuicultura como suministradora de prote{\'i}na animal en el mundo}},
  url = {https://www.ipacuicultura.com/noticia-68513-seccion-Internacional},
  urldate = {2024-02-27},
  abstract = {QU Dongyu realiz{\'o} una visita a la Universidad Estatal de Mississippi para conocer los proyectos en los que ambas entidades colaboran, entre ellos en el {\'a}mbito de la acuicultura},
  langid = {spanish}
}

@misc{dmcadminQueEsAcuicultura2022,
  title = {{{\textquestiondown}Qu{\'e} es la acuicultura?}},
  author = {{dmcadmin}},
  year = {2022},
  month = jun,
  journal = {Domca},
  url = {https://www.domca.com/que-es-la-acuicultura/},
  urldate = {2024-04-01},
  abstract = {{\textquestiondown}Conoces los mejores m{\'e}todos de conservaci{\'o}n de alimentos? En Domca desarrollamos productos para aumentar la vida {\'u}til de los alimentos},
  chapter = {Blog},
  langid = {spanish}
}

@book{duNeuralNetworksStatistical2013,
  title = {Neural {{Networks}} and {{Statistical Learning}}},
  author = {Du, Ke-Lin and Swamy, M.N.s},
  year = {2013},
  month = oct,
  journal = {Neural Networks and Statistical Learning},
  doi = {10.1007/978-1-4471-5571-3},
  abstract = {Providing a broad but in-depth introduction to neural network and machine learning in a statistical framework, this book provides a single, comprehensive resource for study and further research. All the major popular neural network models and statistical learning approaches are covered with examples and exercises in every chapter to develop a practical working understanding of the content. Each of the twenty-five chapters includes state-of-the-art descriptions and important research results on the respective topics. The broad coverage includes the multilayer perceptron, the Hopfield network, associative memory models, clustering models and algorithms, the radial basis function network, recurrent neural networks, principal component analysis, nonnegative matrix factorization, independent component analysis, discriminant analysis, support vector machines, kernel methods, reinforcement learning, probabilistic and Bayesian networks, data fusion and ensemble learning, fuzzy sets and logic, neurofuzzy models, hardware implementations, and some machine learning topics. Applications to biometric/bioinformatics and data mining are also included. Focusing on the prominent accomplishments and their practical aspects, academic and technical staff, graduate students and researchers will find that this provides a solid foundation and encompassing reference for the fields of neural networks, pattern recognition, signal processing, machine learning, computational intelligence, and data mining.},
  isbn = {978-1-4471-5570-6}
}

@misc{EstadoMundialPesca,
  title = {{El estado mundial de la pesca y la acuicultura 2022}},
  doi = {10.4060/cc0461es},
  url = {https://www.fao.org/3/cc0461es/online/cc0461es.html},
  urldate = {2024-02-27},
  abstract = {Hacia la transformaci{\'o}n azul},
  langid = {spanish}
}

@misc{FlowNetLearningOptical,
  title = {{{FlowNet}}: {{Learning Optical Flow}} with {{Convolutional Networks}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/7410673},
  urldate = {2024-02-21}
}

@article{FlujoOptico2022,
  title = {{Flujo {\'o}ptico}},
  year = {2022},
  month = dec,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Flujo_%C3%B3ptico&oldid=148128892},
  urldate = {2024-02-14},
  abstract = {El flujo {\'o}ptico es el patr{\'o}n del movimiento aparente de los objetos, superficies y bordes en una escena causado por el movimiento relativo entre un observador (un ojo o una c{\'a}mara) y la escena.[2]\hspace{0pt}[3]\hspace{0pt} El concepto de flujo {\'o}ptico se estudi{\'o} por primera vez en la d{\'e}cada de 1940 y, finalmente, fue publicado por el psic{\'o}logo estadounidense  James J. Gibson[4]\hspace{0pt} como parte de su teor{\'i}a de la affordance (una acci{\'o}n que un individuo puede potencialmente realizar en su ambiente). Las aplicaciones del flujo {\'o}ptico tales como la detecci{\'o}n de movimiento, la segmentaci{\'o}n de objetos, el tiempo hasta la colisi{\'o}n y el enfoque de c{\'a}lculo de expansiones, la codificaci{\'o}n del movimiento compensado y la medici{\'o}n de la disparidad estereosc{\'o}pica utilizan este movimiento de las superficies y bordes de los objetos.[5]\hspace{0pt}[6]\hspace{0pt}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {spanish},
  annotation = {Page Version ID: 148128892}
}

@article{friardBORISFreeVersatile2016,
  title = {{{BORIS}}: A Free, Versatile Open-Source Event-Logging Software for Video/Audio Coding and Live Observations},
  shorttitle = {{{BORIS}}},
  author = {Friard, Olivier and Gamba, Marco},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {11},
  pages = {1325--1330},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12584},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12584},
  urldate = {2024-04-04},
  abstract = {Quantitative aspects of the study of animal and human behaviour are increasingly relevant to test hypotheses and find empirical support for them. At the same time, photo and video cameras can store a large number of video recordings and are often used to monitor the subjects remotely. Researchers frequently face the need to code considerable quantities of video recordings with relatively flexible software, often constrained by species-specific options or exact settings. BORIS is a free, open-source and multiplatform standalone program that allows a user-specific coding environment to be set for a computer-based review of previously recorded videos or live observations. Being open to user-specific settings, the program allows a project-based ethogram to be defined that can then be shared with collaborators, or can be imported or modified. Projects created in BORIS can include a list of observations, and each observation may include one or two videos (e.g. simultaneous screening of visual stimuli and the subject being tested; recordings from different sides of an aquarium). Once the user has set an ethogram, including state or point events or both, coding can be performed using previously assigned keys on the computer keyboard. BORIS allows definition of an unlimited number of events (states/point events) and subjects. Once the coding process is completed, the program can extract a time-budget or single or grouped observations automatically and present an at-a-glance summary of the main behavioural features. The observation data and time-budget analysis can be exported in many common formats (TSV, CSV, ODF, XLS, SQL and JSON). The observed events can be plotted and exported in various graphic formats (SVG, PNG, JPG, TIFF, EPS and PDF).},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {behaviour coding,behavioural analysis,coding scheme,ethology,observational data,ttime-budget}
}

@inproceedings{gunawanROIYOLOv8BasedFarDistanceFaceRecognition2023,
  title = {{{ROI-YOLOv8-Based Far-Distance Face-Recognition}}},
  booktitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Intelligent Systems}} ({{ARIS}})},
  author = {Gunawan, Felix and Hwang, Chih-Lyang and Cheng, Zih-En},
  year = {2023},
  month = aug,
  pages = {1--6},
  issn = {2572-6919},
  doi = {10.1109/ARIS59192.2023.10268512},
  url = {https://ieeexplore.ieee.org/document/10268512},
  urldate = {2024-02-28},
  abstract = {This paper presents a model for far-distance face recognition using ROI-YOLOv8. We achieve this by training YOLOv8 for 3 target faces on our custom datasets: (i) The first dataset is the original dataset with training and validation images of 2412 and 229, respectively. (ii) The second dataset augments the more pixelated images from the 1st one with training and validation images of 4824 and 458, respectively. (iii) The third dataset considers various exposure, noise, and blur from the 2nd one with training and validation images of 14,272 and 459, respectively. To enhance the far-distance recognition, a two-stage recognition is considered. At first, a pre-trained YOLOv8 model for human detection is achieved. A Region-of-Interest (ROI) including detected humans is segmented as the size of 640 {\texttimes} 640 pixels for the input of another YOLOv8, i.e., ROI-YOLOv8-FR. A computer with Intel i5-12400F, 16GB RAM, and NVIDIA RTX 3080Ti with 12GB VRAM is used as computing platform. The trained times for these 3 datasets are 50, 95, and 219 minutes, respectively. Their mAP50s are 99.5\% and mAP50-95s are slightly different as 88.112\%, 87.962\%, and 88.103\% respectively. More important, the 3rd trained model can successfully recognize a face at 30 and 35m with the confidences of 65.1\% and 75.6\%.},
  keywords = {Computational modeling,Face recognition,Far-distance face-recognition,Image quality,Image segmentation,Intelligent systems,Random access memory,ROI,Training,YOLOv8}
}

@article{hanAdvancedDeepLearningTechniques2018,
  title = {Advanced {{Deep-Learning Techniques}} for {{Salient}} and {{Category-Specific Object Detection}}: {{A Survey}}},
  shorttitle = {Advanced {{Deep-Learning Techniques}} for {{Salient}} and {{Category-Specific Object Detection}}},
  author = {Han, Junwei and Zhang, Dingwen and Cheng, Gong and Liu, Nian and Xu, Dong},
  year = {2018},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {84--100},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2749125},
  url = {https://ieeexplore.ieee.org/document/8253582},
  urldate = {2024-02-14},
  abstract = {Object detection, including objectness detection (OD), salient object detection (SOD), and category-specific object detection (COD), is one of the most fundamental yet challenging problems in the computer vision community. Over the last several decades, great efforts have been made by researchers to tackle this problem, due to its broad range of applications for other computer vision tasks such as activity or event recognition, content-based image retrieval and scene understanding, etc. While numerous methods have been presented in recent years, a comprehensive review for the proposed high-quality object detection techniques, especially for those based on advanced deep-learning techniques, is still lacking. To this end, this article delves into the recent progress in this research field, including 1) definitions, motivations, and tasks of each subdirection; 2) modern techniques and essential research trends; 3) benchmark data sets and evaluation metrics; and 4) comparisons and analysis of the experimental results. More importantly, we will reveal the underlying relationship among OD, SOD, and COD and discuss in detail some open questions as well as point out several unsolved challenges and promising future works.},
  keywords = {Computer architecture,Computer vision,Convolution,Feature extraction,Machine learning,Object detection,Visualization}
}

@article{hornDeterminingOpticalFlow1981,
  title = {Determining {{Optical Flow}}},
  author = {Horn, Berthold and Schunck, Brian},
  year = {1981},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {17},
  pages = {185--203},
  doi = {10.1016/0004-3702(81)90024-2},
  abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.}
}

@misc{IntroductionKMeansClustering,
  title = {Introduction to K-{{Means Clustering}} with Scikit-Learn in {{Python}}},
  url = {https://www.datacamp.com/tutorial/k-means-clustering-python},
  urldate = {2024-05-21},
  abstract = {In this tutorial, learn how to apply k-Means Clustering with scikit-learn in Python},
  langid = {english}
}

@inproceedings{liAcceleratingFishDetection2016,
  title = {Accelerating Fish Detection and Recognition by Sharing {{CNNs}} with Objectness Learning},
  booktitle = {{{OCEANS}} 2016 - {{Shanghai}}},
  author = {Li, Xiu and Shang, Min and Hao, Jing and Yang, Zhixiong},
  year = {2016},
  month = apr,
  pages = {1--5},
  doi = {10.1109/OCEANSAP.2016.7485476},
  url = {https://ieeexplore.ieee.org/document/7485476},
  urldate = {2024-03-06},
  abstract = {Daily increasing underwater visual data makes automatic object detection and recognition a great demand and challenging task. In this paper, we adopt a region proposal network to accelerate underwater object detection and recognition from Faster R-CNN. This process implement detection acceleration by using convolutional networks to generate high-quality object candidates, and by sharing these networks with original detection networks. Applied to a real-world fish dataset, which includes 24,277 ImageCLEF fish images belonging to 12 classes, this automatic detection and recognition system has a nearly real-time frame rate of 9.8 ftps, while yielding 15.1\% higher Mean Average Precision (mAP) than the Deformable Parts Model (DPM) baseline.},
  keywords = {Detection acceleration,Faster R-CNN,Feature extraction,Fish,high-quality proposals,Image segmentation,Object detection,Proposals,Training,Underwater images,Visualization}
}

@inproceedings{manojkumarPerformanceComparisonReal2023,
  title = {Performance {{Comparison}} of {{Real Time Object Detection Techniques}} with {{YOLOv4}}},
  booktitle = {2023 {{International Conference}} on {{Signal Processing}}, {{Computation}}, {{Electronics}}, {{Power}} and {{Telecommunication}} ({{IConSCEPT}})},
  author = {Manojkumar, P C and Kumar, Lakshmi Sutha and Jayanthi, B},
  year = {2023},
  month = may,
  pages = {1--6},
  doi = {10.1109/IConSCEPT57958.2023.10169970},
  url = {https://ieeexplore.ieee.org/document/10169970},
  urldate = {2024-02-14},
  abstract = {Computer vision is a recent technological advancement to digitally perceive the real world at an advanced level, through digital images and videos. Object detection is a subset of computer vision which is one of the prominent techniques used for object tracking, automatic driving, anomaly detection, etc. Object detection can be based on either machine learning or deep learning algorithms, it can be used for the localization of the image and classification of elements into diverse classes. This work provides a comparison of the object detection approaches such as Region with Convolutional Neural Network (R-CNN), Fast R-CNN, and You Only Look Once(YOLO) and Single Shot multibox Detector (SSD). The implementation of an object detection technique YOLOv4 and a custom model are done, which recognizes the objects from an input image, webcam image and live stream webcam video.},
  keywords = {COCO,Computer vision,Deep learning,Inference algorithms,Object detection,real-time object detection,Signal processing,Signal processing algorithms,Streaming media,Webcams,YOLO,YOLOv4}
}

@misc{MultiprocessingProcessbasedParallelism,
  title = {Multiprocessing --- {{Process-based}} Parallelism --- {{Python}} 3.10.13 Documentation},
  url = {https://docs.python.org/3.10/library/multiprocessing.html#synchronization-between-processes},
  urldate = {2024-05-27}
}

@misc{NEMESIS3DCM,
  title = {{NEMESIS-3D-CM}},
  url = {http://nemesis3d.citsem.upm.es/inicio/},
  urldate = {2024-06-09},
  langid = {spanish}
}

@inproceedings{nomanObjectDetectionTechniques2019,
  title = {Object {{Detection Techniques}}: {{Overview}} and {{Performance Comparison}}},
  shorttitle = {Object {{Detection Techniques}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Signal Processing}} and {{Information Technology}} ({{ISSPIT}})},
  author = {Noman, Mohammed and Stankovic, Vladimir and Tawfik, Ayman},
  year = {2019},
  month = dec,
  pages = {1--5},
  issn = {2641-5542},
  doi = {10.1109/ISSPIT47144.2019.9001879},
  url = {https://ieeexplore.ieee.org/document/9001879},
  urldate = {2024-02-14},
  abstract = {Object detection algorithms are improving by the minute. There are many common libraries or application program interfaces (APIs) to use. The most two common ones are Microsoft Azure Cloud object detection and Google Tensorflow object detection. The first is an online-network based API, while the second is an offline machine-based API. Both have their advantages and disadvantages. A direct comparison between the most common object detection methods helps in finding the best solution for advance system integration. This paper will discuss both methods and compare them in terms of accuracy, complexity and practicality. It will show advantages and also limitations of each method, and possibilities for improvement.},
  keywords = {Azure Cloud,Computer vision,Feature extraction,Libraries,Object detection,Standards,Tensorflow,Testing,Training}
}

@misc{ObservatorioUPM,
  title = {Observatorio de {{I}}+{{D}}+i {{UPM}}},
  url = {https://www.upm.es/observatorio/vi/index.jsp?pageac=estructuras/grupo.jsp&idGrupo=177&h=1},
  urldate = {2024-06-01}
}

@article{pertuzAnalysisFocusMeasure2013,
  title = {Analysis of Focus Measure Operators for Shape-from-Focus},
  author = {Pertuz, Said and Puig, Domenec and Garcia, Miguel Angel},
  year = {2013},
  month = may,
  journal = {Pattern Recognition},
  volume = {46},
  number = {5},
  pages = {1415--1432},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.11.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320312004736},
  urldate = {2024-05-21},
  abstract = {Shape-from-focus (SFF) has widely been studied in computer vision as a passive depth recovery and 3D reconstruction method. One of the main stages in SFF is the computation of the focus level for every pixel of an image by means of a focus measure operator. In this work, a methodology to compare the performance of different focus measure operators for shape-from-focus is presented and applied. The selected operators have been chosen from an extensive review of the state-of-the-art. The performance of the different operators has been assessed through experiments carried out under different conditions, such as image noise level, contrast, saturation and window size. Such performance is discussed in terms of the working principles of the analyzed operators.},
  keywords = {Autofocus,Defocus model,Focus measure,Shape from focus}
}

@misc{prakashThreadingVsMultiprocessing2023,
  title = {Threading vs {{Multiprocessing}} in {{Python}}: {{A Comprehensive Guide}}},
  shorttitle = {Threading vs {{Multiprocessing}} in {{Python}}},
  author = {Prakash, Arjun},
  year = {2023},
  month = jun,
  journal = {Medium},
  url = {https://medium.com/@arjunprakash027/threading-vs-multiprocessing-in-python-a-comprehensive-guide-cae3ce0ca6c1},
  urldate = {2024-05-27},
  abstract = {Introduction},
  langid = {english}
}

@article{ramzanReviewStateoftheArtViolence2019,
  title = {A {{Review}} on {{State-of-the-Art Violence Detection Techniques}}},
  author = {Ramzan, Muhammad and Abid, Adnan and Khan, Hikmat Ullah and Awan, Shahid Mahmood and Ismail, Amina and Ahmed, Muzamil and Ilyas, Mahwish and Mahmood, Ahsan},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {107560--107575},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2932114},
  url = {https://ieeexplore.ieee.org/document/8782115},
  urldate = {2024-02-14},
  abstract = {With the rapid growth of surveillance cameras to monitor the human activity demands such system which recognize the violence and suspicious events automatically. Abnormal and violence action detection has become an active research area of computer vision and image processing to attract new researchers. The relevant literature presents different techniques for detection of such activities from the video proposed in the recent years. This paper reviews various state-of-the-art techniques of violence detection. In this paper, the methods of detection are divided into three categories that is based on classification techniques used: violence detection using traditional machine learning, using support vector machine (SVM), and using deep learning. The feature extraction techniques and object detection techniques of the each single method are also presented. Moreover, datasets and video features that used in the techniques, which play a vital role in recognition process are also discussed. For better understanding, the steps of the research approaches have been presented in an architecture diagram. The overall research findings have been discussed which may be helpful for finding the potential future work in this research domain.},
  keywords = {Bibliographies,Cameras,computer vision,Computer vision,deep learning,Feature extraction,Libraries,machine learning,support vector machine,Surveillance,surveillance camera,Systematics,Violence detection,violent behavior}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  pages = {779--788},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.91},
  url = {https://ieeexplore.ieee.org/document/7780460},
  urldate = {2024-03-06},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  keywords = {Computer architecture,Microprocessors,Neural networks,Object detection,Pipelines,Real-time systems,Training}
}

@article{sharmaAgileProcessesMethodologies2012,
  title = {Agile {{Processes}} and {{Methodologies}}: {{A Conceptual Study}}},
  shorttitle = {Agile {{Processes}} and {{Methodologies}}},
  author = {Sharma, Sheetal and Sarkar, Darothi and Gupta, Divya},
  year = {2012},
  month = may,
  journal = {International Journal on Computer Science and Engineering},
  volume = {4},
  abstract = {This paper deals with the comparative study of agile processes. The paper will serve as guide to other software development process models. Agile processes have important applications in the areas of software project management, software schedule management, etc. In particular the aim of agile processes is to satisfy the customer, faster development times with lower defects rate. This paper compares the agile processes with other software development life cycle models. Agile processes are not always advantageous, they have some drawbacks as well; the advantages and disadvantages of agile processes are also discussed in this paper.}
}

@book{szeliskiComputerVisionAlgorithms2022,
  title = {Computer Vision: Algorithms and Applications},
  shorttitle = {Computer Vision},
  author = {Szeliski, Richard},
  year = {2022},
  publisher = {Springer Nature},
  url = {https://books.google.es/books?hl=es&lr=&id=QptXEAAAQBAJ&oi=fnd&pg=PR9&dq=computer+vision+algorithms+and+applications+2nd+ed&ots=BNvitZVCsk&sig=ecxTFEpxbEYtL2yzTq0MHzbPPpI},
  urldate = {2024-06-09}
}

@inproceedings{ulucanLargeScaleDatasetFish2020,
  title = {A {{Large-Scale Dataset}} for {{Fish Segmentation}} and {{Classification}}},
  booktitle = {2020 {{Innovations}} in {{Intelligent Systems}} and {{Applications Conference}} ({{ASYU}})},
  author = {Ulucan, Oguzhan and Karakaya, Diclehan and Turkan, Mehmet},
  year = {2020},
  month = oct,
  pages = {1--5},
  doi = {10.1109/ASYU50717.2020.9259867},
  url = {https://ieeexplore.ieee.org/document/9259867},
  urldate = {2024-03-06},
  abstract = {Assessing the quality of seafood both in retail and during packaging at the production side must be carried out minutely in order to avoid spoilage which causes severe human health problems and also economic loss. Since the illnesses and decay in seafood presents distinct symptoms in different species, primarily the classification of species is required. In this field, the inadequacy of the current laborious and slow traditional methods can be overcome with systems based on machine learning and image processing, which present fast and precise results. In order design such systems, practical and suitable datasets are required. However, most of the publicly available datasets are not fit for the mentioned purpose. These datasets either contain images taken underwater or consist of seafood which is generally not (widely) consumed. In this study, a practical and large dataset containing nine distinct seafood widely consumed in the Aegean Region of Turkey is formed. Additionally, comprehensive experiments based on different classification approaches are performed to analyze the usability of this collected dataset. Experimental results demonstrate very promising outcomes; therefore, this dataset will be made publicly available for further investigations in this research domain.},
  keywords = {classification,Economics,feature extraction,Feature extraction,Fish dataset,food quality assessment,Image color analysis,Image segmentation,Quality assessment,segmentation,Task analysis,Training}
}

@misc{YOLOv5VsYOLOv62022,
  title = {{{YOLOv5}} vs {{YOLOv6}} vs {{YOLOv7}}: {{Comparison}} of {{YOLO Models}} on {{Speed}} and {{Accuracy}} {\textbar} {{CPU}} \& {{GPU}}},
  shorttitle = {{{YOLOv5}} vs {{YOLOv6}} vs {{YOLOv7}}},
  year = {2022},
  month = nov,
  url = {https://learnopencv.com/performance-comparison-of-yolo-models/},
  urldate = {2024-06-05},
  abstract = {We present a etailed Comparison of YOLO Models. Which YOLO model is the fastest? What about inference speed on CPU vs GPU? Which YOLO model is the most accurate?},
  langid = {american}
}

@article{zouObjectDetection202023,
  title = {Object Detection in 20 Years: {{A}} Survey},
  shorttitle = {Object Detection in 20 Years},
  author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  year = {2023},
  journal = {Proceedings of the IEEE},
  volume = {111},
  number = {3},
  pages = {257--276},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10028728/},
  urldate = {2024-06-09}
}

@inproceedings{zouReviewObjectDetection2019,
  title = {A {{Review}} of {{Object Detection Techniques}}},
  booktitle = {2019 {{International Conference}} on {{Smart Grid}} and {{Electrical Automation}} ({{ICSGEA}})},
  author = {Zou, Xinrui},
  year = {2019},
  month = aug,
  pages = {251--254},
  doi = {10.1109/ICSGEA.2019.00065},
  url = {https://ieeexplore.ieee.org/abstract/document/8901325},
  urldate = {2024-02-07},
  abstract = {Object detection is widely used in the field of computer vision and crucial for variety of applications, e.g., self-driving car. During the development of half a century, object detection methods have been continuously developed, and generated numerous approaches which obtained promising achievements. At present, the approach of object detection has been largely evolved into two categories which are traditional machine learning methods utilizing varied computer vision techniques and deep learning method. This article presents a review of object detection techniques. Firstly, the existing methods based on traditional machine learning are summarized and introduced. Then, two main schools of deep learning methods, R-CNN and YOLO, are selected for analysis and introduction. At the end of the article, the methods mentioned are briefly compared and discussed.},
  keywords = {Computer Vision,Machine Learning,Target Detection}
}
