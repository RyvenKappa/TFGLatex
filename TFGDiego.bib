@misc{Acuicultura,
  title = {Acuicultura 4.0},
  url = {https://cetga.org/cuatropuntocero/},
  urldate = {2024-06-13}
}

@misc{barriossanchezPruebaRedEvaluando2023,
  type = {{info:eu-repo/semantics/bachelorThesis}},
  title = {{La prueba de la red: evaluando su repetibilidad y el efecto del ayuno en la trucha arcoiris (Oncorhynchus mykiss)}},
  shorttitle = {{La prueba de la red}},
  author = {Barrios S{\'a}nchez, Juan Enrique},
  year = {2023},
  month = nov,
  publisher = {E.T.S. de Ingenier{\'i}a Agron{\'o}mica, Alimentaria y de Biosistemas (UPM)},
  address = {Madrid},
  url = {https://oa.upm.es/77142/},
  urldate = {2024-04-04},
  abstract = {El bienestar animal y la optimizaci{\'o}n de procesos productivos se ha convertido en un punto clave en la industria animal debido a la incipiente concienciaci{\'o}n por parte de los consumidores. La importancia de la industria acu{\'i}cola en la producci{\'o}n animal radica, entre otras cosas, en su valor como fuente proteica. Es por ello que el n{\'u}mero de art{\'i}culos cient{\'i}ficos relacionados con la etolog{\'i}a y el bienestar en peces ha aumentado de manera significativa en los {\'u}ltimos a{\~n}os, al igual que en el resto de especies zoot{\'e}cnicas. Sin embargo, la literatura existente acerca de la eficacia de los test de personalidad con el fin de mejorar las caracter{\'i}sticas productivas es escasa. En este Trabajo Fin de Grado se han estudiado diversos aspectos (repetibilidad, valoraci{\'o}n de la personalidad, efecto del ayuno y descanso entre movimientos) de la prueba de la red (net-test), la cual se usa normalmente para diferenciar entre peces proactivos y reactivos. La prueba consiste en extraer y mantener al pez fuera del agua con una red durante un determinado tiempo para cuantificar los movimientos de escape o saltos. Se utilizaron 90 truchas arco{\'i}ris (Oncorhynchus mykiss) con un peso medio de 368 {\textpm} 43,44 g, separadas en nueve grupos (10 peces por tanque). Una vez a la semana durante un mes (28 d{\'i}as) los peces fueron sometidos al test durante 15 segundos, extrayendo a los individuos de manera individual. La catalogaci{\'o}n conductual basada en los movimientos se dividi{\'o} en proactivos, para los peces que realizaban mayor n{\'u}mero de movimientos, reactivos para los que registraban un n{\'u}mero de movimientos inferior a la media, y neutral para los peces que se encontraban en la media. En la tercera semana se aplicaron tres tratamientos de ayuno: tres grupos control de peces sin restricci{\'o}n alimentaria; tres grupos con un ayuno inferior a 60 grados d{\'i}a (4 d{\'i}as y 54ºC d); y otros tres grupos con un ayuno superior a 80 grados d{\'i}a (7 d{\'i}as y 94,5ºC d). Se confirm{\'o} la repetibilidad de la prueba al no existir diferencias significativas a lo largo de los test. Asimismo, la reducci{\'o}n del tiempo de prueba a 10 segundos demostr{\'o} ser un buen modelo para dictaminar la personalidad del pez frente a la reducci{\'o}n de 5 segundos, coincidiendo en que la categor{\'i}a neutra podr{\'i}a afectar a la precisi{\'o}n y sensibilidad. Adem{\'a}s, se realiz{\'o} un PCA, incluyendo las variables de latencia de escape (PC1 = -0,336), tiempo total escapando (PC1 = 0,754) y el n{\'u}mero total de movimientos (PC1 = 0,704), observ{\'a}ndose que los peces con una latencia de escape m{\'a}s peque{\~n}a, presentaron valores m{\'a}s altos en las otras dos variables, siendo estos peces m{\'a}s proactivos. Se observ{\'o} que el ayuno no present{\'o} diferencias significativas en el n{\'u}mero de movimientos, lo que supone que el test se puede validar en cualquier estado de alimentaci{\'o}n. Respecto a las franjas de descanso de los peces dentro de la red, no se apreciaron diferencias significativas al aumentar el tiempo de las pruebas, de forma que no influy{\'o} sobre el estudio de personalidad en peces. En conclusi{\'o}n, se observ{\'o} que la prueba de la red presenta resultados similares a lo largo del tiempo, pudiendo categorizar a los individuos seg{\'u}n su conducta y bajo cualquier r{\'e}gimen alimenticio, recomendando una duraci{\'o}n {\'o}ptima de entre 10 y 15 segundos en futuras investigaciones etol{\'o}gicas.},
  collaborator = {Villarroel Robinson, Morris Ricardo and de la LLave Prop{\'i}n, {\'A}lvaro},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/3.0/es/},
  langid = {spanish}
}

@inproceedings{bhagyaOverviewDeepLearning2019,
  title = {An {{Overview}} of {{Deep Learning Based Object Detection Techniques}}},
  booktitle = {2019 1st {{International Conference}} on {{Innovations}} in {{Information}} and {{Communication Technology}} ({{ICIICT}})},
  author = {Bhagya, C. and Shyna, A.},
  year = {2019},
  month = apr,
  pages = {1--6},
  doi = {10.1109/ICIICT1.2019.8741359},
  url = {https://ieeexplore.ieee.org/document/8741359},
  urldate = {2024-02-14},
  abstract = {Recent years have witnessed a boundless growth in the field of deep learning. With the preferment in the field of deep learning, the task of object detection has become more exciting and challenging. Object detection focuses on detecting the presence of entire objects within a given image. Deep learning based object detection techniques have shown an efficacy to learn the object features directly from the data. The paper mainly focuses on providing a survey on various state-of-the-art deep learning based object detection techniques. The work also concentrates on providing an extensive comparison regarding the opportunities and obstacles faced by different object detection techniques. The paper concludes by identifying the future golden scopes for research in these fields.},
  keywords = {Computer vision,Deep learning,Feature extraction,Microsoft Windows,Object detection,Proposals,Task analysis}
}

@misc{castillomoralSistemaConteoPeces2022,
  title = {{Sistema de conteo de peces en piscifactor{\'i}as mediante redes neuronales}},
  author = {Castillo Moral, Mar{\'i}a},
  year = {2022},
  journal = {Sistema de conteo de peces en piscifactor{\'i}as mediante redes neuronales},
  publisher = {M. Castillo Moral},
  address = {Madrid]},
  collaborator = {Guti{\'e}rrez Arriola, Juana M. and {Universidad Polit{\'e}cnica de Madrid Escuela T{\'e}cnica Superior de Ingenier{\'i}a y Sistemas de Telecomunicaci{\'o}n} and {Universidad Polit{\'e}cnica de Madrid Departamento de Ingenier{\'i}a Telem{\'a}tica y Electr{\'o}nica}},
  langid = {spanish},
  lccn = {G-22 C1230D CAS SIS, CD G-22 C1230D CAS SIS},
  keywords = {Redes neuronales (Informatica); Reconocimiento de formas; Acuicultura; Trabajos fin de grado}
}

@article{chuangTrackingLiveFish2015,
  title = {Tracking {{Live Fish From Low-Contrast}} and {{Low-Frame-Rate Stereo Videos}}},
  author = {Chuang, Meng-Che and Hwang, Jenq-Neng and Williams, Kresimir and Towler, Richard},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {25},
  number = {1},
  pages = {167--179},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2014.2357093},
  url = {https://ieeexplore.ieee.org/document/6898002},
  urldate = {2024-03-06},
  abstract = {Nonextractive fish abundance estimation with the aid of visual analysis has drawn increasing attention. Unstable illumination, ubiquitous noise, and low-frame-rate (LFR) video capturing in the underwater environment, however, make conventional tracking methods unreliable. In this paper, we present a multiple fish-tracking system for low-contrast and LFR stereo videos with the use of a trawl-based underwater camera system. An automatic fish segmentation algorithm overcomes the low-contrast issues by adopting a histogram backprojection approach on double local-thresholded images to ensure an accurate segmentation on the fish shape boundaries. Built upon a reliable feature-based object matching method, a multiple-target tracking algorithm via a modified Viterbi data association is proposed to overcome the poor motion continuity and frequent entrance/exit of fish targets under LFR scenarios. In addition, a computationally efficient block-matching approach performs successful stereo matching that enables an automatic fish-body tail compensation to greatly reduce segmentation error and allows for an accurate fish length measurement. Experimental results show that an effective and reliable tracking performance for multiple live fish with underwater stereo cameras is achieved.},
  keywords = {Cameras,Estimation,Fish abundance estimation,low-frame-rate (LFR) video,Marine animals,multiple target tracking,stereo imaging,Stereo vision,Target tracking,underwater video,Videos}
}

@misc{ContourFeaturesOpenCVPython,
  title = {Contour {{Features}} --- {{OpenCV-Python Tutorials}} Beta Documentation},
  url = {https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.html},
  urldate = {2024-02-22}
}

@misc{ConvolutionalNeuralNetwork,
  title = {Convolutional {{Neural Network}} ({{CNN}})},
  journal = {NVIDIA Developer},
  url = {https://developer.nvidia.com/discover/convolutional-neural-network},
  urldate = {2024-06-12},
  langid = {american}
}

@misc{CUDAProgrammingGuide,
  title = {{{CUDA C}}++ {{Programming Guide}}},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities},
  urldate = {2024-03-07}
}

@misc{cvat.aicorporationComputerVisionAnnotation2023,
  title = {Computer {{Vision Annotation Tool}} ({{CVAT}})},
  author = {{CVAT.ai Corporation}},
  year = {2023},
  month = nov,
  url = {https://github.com/cvat-ai/cvat},
  urldate = {2024-06-25},
  abstract = {Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.},
  copyright = {MIT}
}

@misc{DirectorGeneralFAO,
  title = {{El director general de la FAO destaca el papel esencial de la acuicultura como suministradora de prote{\'i}na animal en el mundo}},
  url = {https://www.ipacuicultura.com/noticia-68513-seccion-Internacional},
  urldate = {2024-02-27},
  abstract = {QU Dongyu realiz{\'o} una visita a la Universidad Estatal de Mississippi para conocer los proyectos en los que ambas entidades colaboran, entre ellos en el {\'a}mbito de la acuicultura},
  langid = {spanish}
}

@misc{dmcadminQueEsAcuicultura2022,
  title = {{{\textquestiondown}Qu{\'e} es la acuicultura?}},
  author = {{dmcadmin}},
  year = {2022},
  month = jun,
  journal = {Domca},
  url = {https://www.domca.com/que-es-la-acuicultura/},
  urldate = {2024-04-01},
  abstract = {{\textquestiondown}Conoces los mejores m{\'e}todos de conservaci{\'o}n de alimentos? En Domca desarrollamos productos para aumentar la vida {\'u}til de los alimentos},
  chapter = {Blog},
  langid = {spanish}
}

@article{doBasicsDeepLearning2020,
  title = {Basics of {{Deep Learning}}: {{A Radiologist}}'s {{Guide}} to {{Understanding Published Radiology Articles}} on {{Deep Learning}}},
  shorttitle = {Basics of {{Deep Learning}}},
  author = {Do, Synho and Song, Kyoung and Chung, Joo},
  year = {2020},
  month = jan,
  journal = {Korean journal of radiology},
  volume = {21},
  pages = {33--41},
  doi = {10.3348/kjr.2019.0312},
  abstract = {Artificial intelligence has been applied to many industries, including medicine. Among the various techniques in artificial intelligence, deep learning has attained the highest popularity in medical imaging in recent years. Many articles on deep learning have been published in radiologic journals. However, radiologists may have difficulty in understanding and interpreting these studies because the study methods of deep learning differ from those of traditional radiology. This review article aims to explain the concepts and terms that are frequently used in deep learning radiology articles, facilitating general radiologists' understanding.}
}

@misc{doleronDeepLearningScratch2023,
  title = {Deep {{Learning}} from {{Scratch}} in {{Modern C}}++: {{Gradient Descent}}},
  shorttitle = {Deep {{Learning}} from {{Scratch}} in {{Modern C}}++},
  author = {{doleron}, Luiz},
  year = {2023},
  month = aug,
  journal = {Medium},
  url = {https://pub.towardsai.net/deep-learning-from-scratch-in-modern-c-gradient-descent-670bc5889112},
  urldate = {2024-06-26},
  abstract = {Let's have fun by implementing Gradient Descent in pure C++ and Eigen.},
  langid = {english}
}

@book{duNeuralNetworksStatistical2013,
  title = {Neural {{Networks}} and {{Statistical Learning}}},
  author = {Du, Ke-Lin and Swamy, M.N.s},
  year = {2013},
  month = oct,
  journal = {Neural Networks and Statistical Learning},
  doi = {10.1007/978-1-4471-5571-3},
  abstract = {Providing a broad but in-depth introduction to neural network and machine learning in a statistical framework, this book provides a single, comprehensive resource for study and further research. All the major popular neural network models and statistical learning approaches are covered with examples and exercises in every chapter to develop a practical working understanding of the content. Each of the twenty-five chapters includes state-of-the-art descriptions and important research results on the respective topics. The broad coverage includes the multilayer perceptron, the Hopfield network, associative memory models, clustering models and algorithms, the radial basis function network, recurrent neural networks, principal component analysis, nonnegative matrix factorization, independent component analysis, discriminant analysis, support vector machines, kernel methods, reinforcement learning, probabilistic and Bayesian networks, data fusion and ensemble learning, fuzzy sets and logic, neurofuzzy models, hardware implementations, and some machine learning topics. Applications to biometric/bioinformatics and data mining are also included. Focusing on the prominent accomplishments and their practical aspects, academic and technical staff, graduate students and researchers will find that this provides a solid foundation and encompassing reference for the fields of neural networks, pattern recognition, signal processing, machine learning, computational intelligence, and data mining.},
  isbn = {978-1-4471-5570-6}
}

@misc{EstadoMundialPesca,
  title = {{El estado mundial de la pesca y la acuicultura 2022}},
  doi = {10.4060/cc0461es},
  url = {https://www.fao.org/3/cc0461es/online/cc0461es.html},
  urldate = {2024-02-27},
  abstract = {Hacia la transformaci{\'o}n azul},
  langid = {spanish}
}

@misc{FlowNetLearningOptical,
  title = {{{FlowNet}}: {{Learning Optical Flow}} with {{Convolutional Networks}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/7410673},
  urldate = {2024-02-21}
}

@article{FlujoOptico2022,
  title = {{Flujo {\'o}ptico}},
  year = {2022},
  month = dec,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Flujo_%C3%B3ptico&oldid=148128892},
  urldate = {2024-02-14},
  abstract = {El flujo {\'o}ptico es el patr{\'o}n del movimiento aparente de los objetos, superficies y bordes en una escena causado por el movimiento relativo entre un observador (un ojo o una c{\'a}mara) y la escena.[2]\hspace{0pt}[3]\hspace{0pt} El concepto de flujo {\'o}ptico se estudi{\'o} por primera vez en la d{\'e}cada de 1940 y, finalmente, fue publicado por el psic{\'o}logo estadounidense  James J. Gibson[4]\hspace{0pt} como parte de su teor{\'i}a de la affordance (una acci{\'o}n que un individuo puede potencialmente realizar en su ambiente). Las aplicaciones del flujo {\'o}ptico tales como la detecci{\'o}n de movimiento, la segmentaci{\'o}n de objetos, el tiempo hasta la colisi{\'o}n y el enfoque de c{\'a}lculo de expansiones, la codificaci{\'o}n del movimiento compensado y la medici{\'o}n de la disparidad estereosc{\'o}pica utilizan este movimiento de las superficies y bordes de los objetos.[5]\hspace{0pt}[6]\hspace{0pt}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {spanish},
  annotation = {Page Version ID: 148128892}
}

@article{friardBORISFreeVersatile2016,
  title = {{{BORIS}}: A Free, Versatile Open-Source Event-Logging Software for Video/Audio Coding and Live Observations},
  shorttitle = {{{BORIS}}},
  author = {Friard, Olivier and Gamba, Marco},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {11},
  pages = {1325--1330},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12584},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12584},
  urldate = {2024-04-04},
  abstract = {Quantitative aspects of the study of animal and human behaviour are increasingly relevant to test hypotheses and find empirical support for them. At the same time, photo and video cameras can store a large number of video recordings and are often used to monitor the subjects remotely. Researchers frequently face the need to code considerable quantities of video recordings with relatively flexible software, often constrained by species-specific options or exact settings. BORIS is a free, open-source and multiplatform standalone program that allows a user-specific coding environment to be set for a computer-based review of previously recorded videos or live observations. Being open to user-specific settings, the program allows a project-based ethogram to be defined that can then be shared with collaborators, or can be imported or modified. Projects created in BORIS can include a list of observations, and each observation may include one or two videos (e.g. simultaneous screening of visual stimuli and the subject being tested; recordings from different sides of an aquarium). Once the user has set an ethogram, including state or point events or both, coding can be performed using previously assigned keys on the computer keyboard. BORIS allows definition of an unlimited number of events (states/point events) and subjects. Once the coding process is completed, the program can extract a time-budget or single or grouped observations automatically and present an at-a-glance summary of the main behavioural features. The observation data and time-budget analysis can be exported in many common formats (TSV, CSV, ODF, XLS, SQL and JSON). The observed events can be plotted and exported in various graphic formats (SVG, PNG, JPG, TIFF, EPS and PDF).},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {behaviour coding,behavioural analysis,coding scheme,ethology,observational data,ttime-budget}
}

@misc{GPUVsCPU,
  title = {{\textquestiondown}{{GPU}} vs. {{CPU}}? {\textquestiondown}{{Qu{\'e}}} Es La Computaci{\'o}n Por {{GPU}}?~{\textbar}~{{NVIDIA}}},
  url = {https://www.nvidia.com/es-la/drivers/what-is-gpu-computing/},
  urldate = {2024-06-14}
}

@inproceedings{gunawanROIYOLOv8BasedFarDistanceFaceRecognition2023,
  title = {{{ROI-YOLOv8-Based Far-Distance Face-Recognition}}},
  booktitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Intelligent Systems}} ({{ARIS}})},
  author = {Gunawan, Felix and Hwang, Chih-Lyang and Cheng, Zih-En},
  year = {2023},
  month = aug,
  pages = {1--6},
  issn = {2572-6919},
  doi = {10.1109/ARIS59192.2023.10268512},
  url = {https://ieeexplore.ieee.org/document/10268512},
  urldate = {2024-02-28},
  abstract = {This paper presents a model for far-distance face recognition using ROI-YOLOv8. We achieve this by training YOLOv8 for 3 target faces on our custom datasets: (i) The first dataset is the original dataset with training and validation images of 2412 and 229, respectively. (ii) The second dataset augments the more pixelated images from the 1st one with training and validation images of 4824 and 458, respectively. (iii) The third dataset considers various exposure, noise, and blur from the 2nd one with training and validation images of 14,272 and 459, respectively. To enhance the far-distance recognition, a two-stage recognition is considered. At first, a pre-trained YOLOv8 model for human detection is achieved. A Region-of-Interest (ROI) including detected humans is segmented as the size of 640 {\texttimes} 640 pixels for the input of another YOLOv8, i.e., ROI-YOLOv8-FR. A computer with Intel i5-12400F, 16GB RAM, and NVIDIA RTX 3080Ti with 12GB VRAM is used as computing platform. The trained times for these 3 datasets are 50, 95, and 219 minutes, respectively. Their mAP50s are 99.5\% and mAP50-95s are slightly different as 88.112\%, 87.962\%, and 88.103\% respectively. More important, the 3rd trained model can successfully recognize a face at 30 and 35m with the confidences of 65.1\% and 75.6\%.},
  keywords = {Computational modeling,Face recognition,Far-distance face-recognition,Image quality,Image segmentation,Intelligent systems,Random access memory,ROI,Training,YOLOv8}
}

@article{hanAdvancedDeepLearningTechniques2018,
  title = {Advanced {{Deep-Learning Techniques}} for {{Salient}} and {{Category-Specific Object Detection}}: {{A Survey}}},
  shorttitle = {Advanced {{Deep-Learning Techniques}} for {{Salient}} and {{Category-Specific Object Detection}}},
  author = {Han, Junwei and Zhang, Dingwen and Cheng, Gong and Liu, Nian and Xu, Dong},
  year = {2018},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {84--100},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2749125},
  url = {https://ieeexplore.ieee.org/document/8253582},
  urldate = {2024-02-14},
  abstract = {Object detection, including objectness detection (OD), salient object detection (SOD), and category-specific object detection (COD), is one of the most fundamental yet challenging problems in the computer vision community. Over the last several decades, great efforts have been made by researchers to tackle this problem, due to its broad range of applications for other computer vision tasks such as activity or event recognition, content-based image retrieval and scene understanding, etc. While numerous methods have been presented in recent years, a comprehensive review for the proposed high-quality object detection techniques, especially for those based on advanced deep-learning techniques, is still lacking. To this end, this article delves into the recent progress in this research field, including 1) definitions, motivations, and tasks of each subdirection; 2) modern techniques and essential research trends; 3) benchmark data sets and evaluation metrics; and 4) comparisons and analysis of the experimental results. More importantly, we will reveal the underlying relationship among OD, SOD, and COD and discuss in detail some open questions as well as point out several unsolved challenges and promising future works.},
  keywords = {Computer architecture,Computer vision,Convolution,Feature extraction,Machine learning,Object detection,Visualization}
}

@article{hornDeterminingOpticalFlow1981,
  title = {Determining {{Optical Flow}}},
  author = {Horn, Berthold and Schunck, Brian},
  year = {1981},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {17},
  pages = {185--203},
  doi = {10.1016/0004-3702(81)90024-2},
  abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.}
}

@misc{IntroductionKMeansClustering,
  title = {Introduction to K-{{Means Clustering}} with Scikit-Learn in {{Python}}},
  url = {https://www.datacamp.com/tutorial/k-means-clustering-python},
  urldate = {2024-05-21},
  abstract = {In this tutorial, learn how to apply k-Means Clustering with scikit-learn in Python},
  langid = {english}
}

@misc{JgfisherPerceptronPerceptron,
  title = {Jg-Fisher/Perceptron: {{Perceptron}} Model Built from Scratch.},
  url = {https://github.com/jg-fisher/perceptron?tab=readme-ov-file},
  urldate = {2024-06-12}
}

@article{kallfelzsirmacekSEQUENTIALIMAGEPROCESSING2019,
  title = {{{SEQUENTIAL IMAGE PROCESSING METHODS FOR IMPROVING SEMANTIC VIDEO SEGMENTATION ALGORITHMS A PREPRINT}}},
  author = {Kallfelz Sirmacek, Beril and Botteghi, Nicol{\`o} and Sanchez, Santiago},
  year = {2019},
  month = oct,
  abstract = {Recently, semantic video segmentation gained high attention especially for supporting autonomous driving systems. Deep learning methods made it possible to implement real-time segmentation and object identification algorithms on videos. However, most of the available approaches process each video frame independently disregarding their sequential relation in time. Therefore their results suddenly miss some of the object segments in some of the frames even if they were detected properly in the earlier frames. Herein we propose two sequential probabilistic video frame analysis approaches to improve the segmentation performance of the existing algorithms. Our experiments show that using the information of the past frames we increase the performance and consistency of the state of the art algorithms.}
}

@inproceedings{kobayashiAquaColonyFully2023,
  title = {Aqua {{Colony}} for {{Fully Automated Aquaculture}}},
  booktitle = {2023 11th {{IEEE International Conference}} on {{Mobile Cloud Computing}}, {{Services}}, and {{Engineering}} ({{MobileCloud}})},
  author = {Kobayashi, Torn and Tanaka, Yudai and Fukae, Kazuki and Imai, Tetsuo and Arai, Kenichi},
  year = {2023},
  month = jul,
  pages = {11--16},
  issn = {2573-7562},
  doi = {10.1109/MobileCloud58788.2023.00008},
  url = {https://ieeexplore.ieee.org/document/10229441},
  urldate = {2024-06-13},
  abstract = {From the viewpoint of food self-sufficiency, aquaculture is attracting attention. Since aquaculture is a typical labor-intensive industry, it is desirable to improve productivity through the use of IT. Therefore, this paper presents a study of Aqua Colony for fully automated aquaculture. Aqua Colony automates the aquaculture industry, which has relied on manual labor, by utilizing IoT, AI, and drones. Specifically, the AI calculates the optimal amount of food to be fed based on information from the IoT like sensors, and the drone automatically feeds the fish. This paper describes overview of Aqua Colony, and the automatic feeding system, which is characterized by optical flow to a Support Vector Machine (SVM) to determine the degree of fish activity.},
  keywords = {Artificial intelligence,automatic feeding system,Fish,Industries,Manuals,optical flow,Productivity,Sensor phenomena and characterization,Smart aquaculture,Support Vector Machine,Support vector machines}
}

@inproceedings{ladeAutomatedFishSpecies2023,
  title = {Automated {{Fish Species Identification}} Using {{Computer Vision}}},
  booktitle = {2023 7th {{International Conference On Computing}}, {{Communication}}, {{Control And Automation}} ({{ICCUBEA}})},
  author = {Lade, Ritesh and Patil, Atharva and Malu, Kushal and Pal, Sanjivan and Sable, Nilesh and Shelke, Priya},
  year = {2023},
  month = aug,
  pages = {1--8},
  issn = {2771-1358},
  doi = {10.1109/ICCUBEA58933.2023.10392234},
  url = {https://ieeexplore.ieee.org/document/10392234},
  urldate = {2024-06-13},
  abstract = {Indian National Centre for Ocean Information Services (INCOIS) provides Marine Fishery Advisory Services to the fishermen in India. Fisherman in India catches thousands of fishes everyday. Nevertheless, obtaining species-level catch reporting can be difficult for several reasons, including manual efforts that can lead to low or inaccurate reporting. This research suggests an automated fish identification and classification system that uses deep learning algorithms, notably YOLOv5 and ResNet50, to handle this problem. The dataset used in the research was manually created with images were taken from google. Dataset collected from Kaggle was also merged with the created dataset to increase the performance of the system. The system takes in input images of caught fish and accurately detects and classifies the fish species. The proposed system was tested on a dataset of several different species found in Indian subcontinent and the accuracy of 97 and 94 percent was achieved with YOLOv5 and ResNet50 respectively. This shows that YOLOv5 outperformed ResNet50. The results indicate that the proposed system can significantly reduce the time and effort required for species-level fish detection and classification, thus improving the accuracy of fishery advisory services.},
  keywords = {Aquaculture,Computer vision,Image Processing,Marine animals,Process control,Residual neural networks,ResNet50,Surveys,YOLO,Yolov5}
}

@misc{LaplacianFilterOverview,
  title = {Laplacian {{Filter}} - an Overview {\textbar} {{ScienceDirect Topics}}},
  url = {https://www.sciencedirect.com/topics/engineering/laplacian-filter},
  urldate = {2024-06-12}
}

@inproceedings{leeDetectionCatfishActivity2024,
  title = {Detection of {{Catfish Activity}} in {{Smart Fish Farming System}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Consumer Electronics}} ({{ICCE}})},
  author = {Lee, Shih-Hsiung and Ku, Hsuan-Chih and Huang, Ren-Wen},
  year = {2024},
  month = jan,
  pages = {1--3},
  issn = {2158-4001},
  doi = {10.1109/ICCE59016.2024.10444155},
  url = {https://ieeexplore.ieee.org/document/10444155},
  urldate = {2024-06-13},
  abstract = {Fishpond aquaculture plays a critical role in the aquaculture industry. Despite its years of development, it still faces various challenges. One major concern is the occurrence of diseases within the fishpond. These diseases can cause serious harm to the cultivated fish, ultimately affecting the earnings of the aquaculturists. To address this issue, early detection and corresponding measures have become a top priority for fish farmers. One effective approach is through real-time monitoring technology, which allows for the immediate detection of fish behavior and pond water quality. Once anomalies are detected, the farmers can be promptly notified, enabling them to take necessary actions and thereby reduce losses. This paper proposes a novel real-time monitoring system. The system primarily employs cameras, dissolved oxygen meters, and temperature sensors for continuous monitoring of the pond environment and the condition of the cultured fish. It's worth noting that considering the often complex background and target scenes in fishponds, simple target detection alone may not meet the demands of real-time detection. Therefore, this paper combines YOLO-V5s with optical flow technology. This approach not only improves the accuracy of target detection but also better addresses scenarios where water quality in the pond may be compromised. This enhancement not only boosts the accuracy of target detection but also handles various complex situations in the fishpond more effectively.},
  keywords = {Aquaculture,Behavioral sciences,Fish,Monitoring,Object detection,Real-time systems,Water quality}
}

@inproceedings{liAcceleratingFishDetection2016,
  title = {Accelerating Fish Detection and Recognition by Sharing {{CNNs}} with Objectness Learning},
  booktitle = {{{OCEANS}} 2016 - {{Shanghai}}},
  author = {Li, Xiu and Shang, Min and Hao, Jing and Yang, Zhixiong},
  year = {2016},
  month = apr,
  pages = {1--5},
  doi = {10.1109/OCEANSAP.2016.7485476},
  url = {https://ieeexplore.ieee.org/document/7485476},
  urldate = {2024-03-06},
  abstract = {Daily increasing underwater visual data makes automatic object detection and recognition a great demand and challenging task. In this paper, we adopt a region proposal network to accelerate underwater object detection and recognition from Faster R-CNN. This process implement detection acceleration by using convolutional networks to generate high-quality object candidates, and by sharing these networks with original detection networks. Applied to a real-world fish dataset, which includes 24,277 ImageCLEF fish images belonging to 12 classes, this automatic detection and recognition system has a nearly real-time frame rate of 9.8 ftps, while yielding 15.1\% higher Mean Average Precision (mAP) than the Deformable Parts Model (DPM) baseline.},
  keywords = {Detection acceleration,Faster R-CNN,Feature extraction,Fish,high-quality proposals,Image segmentation,Object detection,Proposals,Training,Underwater images,Visualization}
}

@inproceedings{manojkumarPerformanceComparisonReal2023,
  title = {Performance {{Comparison}} of {{Real Time Object Detection Techniques}} with {{YOLOv4}}},
  booktitle = {2023 {{International Conference}} on {{Signal Processing}}, {{Computation}}, {{Electronics}}, {{Power}} and {{Telecommunication}} ({{IConSCEPT}})},
  author = {Manojkumar, P C and Kumar, Lakshmi Sutha and Jayanthi, B},
  year = {2023},
  month = may,
  pages = {1--6},
  doi = {10.1109/IConSCEPT57958.2023.10169970},
  url = {https://ieeexplore.ieee.org/document/10169970},
  urldate = {2024-02-14},
  abstract = {Computer vision is a recent technological advancement to digitally perceive the real world at an advanced level, through digital images and videos. Object detection is a subset of computer vision which is one of the prominent techniques used for object tracking, automatic driving, anomaly detection, etc. Object detection can be based on either machine learning or deep learning algorithms, it can be used for the localization of the image and classification of elements into diverse classes. This work provides a comparison of the object detection approaches such as Region with Convolutional Neural Network (R-CNN), Fast R-CNN, and You Only Look Once(YOLO) and Single Shot multibox Detector (SSD). The implementation of an object detection technique YOLOv4 and a custom model are done, which recognizes the objects from an input image, webcam image and live stream webcam video.},
  keywords = {COCO,Computer vision,Deep learning,Inference algorithms,Object detection,real-time object detection,Signal processing,Signal processing algorithms,Streaming media,Webcams,YOLO,YOLOv4}
}

@article{mccullochLOGICALCALCULUSIDEAS,
  title = {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}}},
  author = {Mcculloch, Warren S and Pitts, Walter},
  langid = {english}
}

@misc{MultiprocessingProcessbasedParallelism,
  title = {Multiprocessing --- {{Process-based}} Parallelism --- {{Python}} 3.10.13 Documentation},
  url = {https://docs.python.org/3.10/library/multiprocessing.html#synchronization-between-processes},
  urldate = {2024-05-27}
}

@misc{NEMESIS3DCM,
  title = {{NEMESIS-3D-CM}},
  url = {http://nemesis3d.citsem.upm.es/inicio/},
  urldate = {2024-06-09},
  langid = {spanish}
}

@misc{ObservatorioUPM,
  title = {Observatorio de {{I}}+{{D}}+i {{UPM}}},
  url = {https://www.upm.es/observatorio/vi/index.jsp?pageac=estructuras/grupo.jsp&idGrupo=177&h=1},
  urldate = {2024-06-01}
}

@article{paiComputerVisionBased2022,
  title = {A {{Computer Vision Based Behavioral Study}} and {{Fish Counting}} in a {{Controlled Environment}}},
  author = {Pai, Krithika M. and Shenoy, K. B. Ajitha and Pai, M. M. Manohara},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {87778--87786},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3197887},
  url = {https://ieeexplore.ieee.org/document/9853540},
  urldate = {2024-06-13},
  abstract = {Aquaculture is a fast-growing food-production sector that accounts for almost 50\% of the world's fish used for consumption. Aquaculture refers to the cultivation of fish in cages. The fish in the cage must be fed at regular intervals, for which the number of fish helps in estimating the amount of feed to be put in the cage. The behavior of fish in a caged environment reflects their health. In the absence of an ambient atmosphere, fish are stressed, which results in frantic movement. The frantic behavior of fish can be identified using recent advancements in image and video processing. In this study, we have focused on frantic behavior detection, fish detection, and counting. For this, a RAS with Tilapia fish has been setup, and the videos of the fish are captured. The detection and counting have been achieved by using the YOLOv5 model. The model has resulted in a Precision, Recall and F-measure of 81\%. The results are compared with the ground truth, which indicates that the model has been successful in counting the fish. The frantic movement of the fish has been detected by developing an optical flow model. The results are encouraging and can be used for frantic behavior detection.},
  keywords = {Aquaculture,Behavioral sciences,Cameras,detection,Fish,Fish counting,frantic behavior,Object detection,Optical flow,Video sequences,Water quality}
}

@misc{PASCALVisualObject,
  title = {The {{PASCAL Visual Object Classes Challenge}} 2012 ({{VOC2012}})},
  url = {http://host.robots.ox.ac.uk/pascal/VOC/voc2012/},
  urldate = {2024-06-13}
}

@article{pertuzAnalysisFocusMeasure2013,
  title = {Analysis of Focus Measure Operators for Shape-from-Focus},
  author = {Pertuz, Said and Puig, Domenec and Garcia, Miguel Angel},
  year = {2013},
  month = may,
  journal = {Pattern Recognition},
  volume = {46},
  number = {5},
  pages = {1415--1432},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.11.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320312004736},
  urldate = {2024-05-21},
  abstract = {Shape-from-focus (SFF) has widely been studied in computer vision as a passive depth recovery and 3D reconstruction method. One of the main stages in SFF is the computation of the focus level for every pixel of an image by means of a focus measure operator. In this work, a methodology to compare the performance of different focus measure operators for shape-from-focus is presented and applied. The selected operators have been chosen from an extensive review of the state-of-the-art. The performance of the different operators has been assessed through experiments carried out under different conditions, such as image noise level, contrast, saturation and window size. Such performance is discussed in terms of the working principles of the analyzed operators.},
  keywords = {Autofocus,Defocus model,Focus measure,Shape from focus}
}

@misc{prakashThreadingVsMultiprocessing2023,
  title = {Threading vs {{Multiprocessing}} in {{Python}}: {{A Comprehensive Guide}}},
  shorttitle = {Threading vs {{Multiprocessing}} in {{Python}}},
  author = {Prakash, Arjun},
  year = {2023},
  month = jun,
  journal = {Medium},
  url = {https://medium.com/@arjunprakash027/threading-vs-multiprocessing-in-python-a-comprehensive-guide-cae3ce0ca6c1},
  urldate = {2024-05-27},
  abstract = {Introduction},
  langid = {english}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  pages = {779--788},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.91},
  url = {https://ieeexplore.ieee.org/document/7780460},
  urldate = {2024-03-06},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  keywords = {Computer architecture,Microprocessors,Neural networks,Object detection,Pipelines,Real-time systems,Training}
}

@misc{sahaComprehensiveGuideConvolutional2022,
  title = {A {{Comprehensive Guide}} to {{Convolutional Neural Networks}} --- the {{ELI5}} Way},
  author = {Saha, Sumit},
  year = {2022},
  month = nov,
  journal = {Medium},
  url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
  urldate = {2024-06-12},
  abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines{\dots}},
  langid = {english}
}

@inproceedings{schusterCombiningStereoDisparity2018,
  title = {Combining {{Stereo Disparity}} and {{Optical Flow}} for {{Basic Scene Flow}}},
  author = {Schuster, Ren{\'e} and Bailer, Christian and Wasenm{\"u}ller, Oliver and Stricker, Didier},
  year = {2018},
  month = mar,
  abstract = {Scene flow is a description of real world motion in 3D that contains more information than optical flow. Because of its complexity there exists no applicable variant for real-time scene flow estimation in an automotive or commercial vehicle context that is sufficiently robust and accurate. Therefore, many applications estimate the 2D optical flow instead. In this paper, we examine the combination of top-performing state-of-the-art optical flow and stereo disparity algorithms in order to achieve a basic scene flow. On the public KITTI Scene Flow Benchmark we demonstrate the reasonable accuracy of the combination approach and show its speed in computation.}
}

@article{sharmaAgileProcessesMethodologies2012,
  title = {Agile {{Processes}} and {{Methodologies}}: {{A Conceptual Study}}},
  shorttitle = {Agile {{Processes}} and {{Methodologies}}},
  author = {Sharma, Sheetal and Sarkar, Darothi and Gupta, Divya},
  year = {2012},
  month = may,
  journal = {International Journal on Computer Science and Engineering},
  volume = {4},
  abstract = {This paper deals with the comparative study of agile processes. The paper will serve as guide to other software development process models. Agile processes have important applications in the areas of software project management, software schedule management, etc. In particular the aim of agile processes is to satisfy the customer, faster development times with lower defects rate. This paper compares the agile processes with other software development life cycle models. Agile processes are not always advantageous, they have some drawbacks as well; the advantages and disadvantages of agile processes are also discussed in this paper.}
}

@book{sonkaImageProcessingAnalysis2013,
  title = {Image {{Processing}}, {{Analysis}} and {{Machine Vision}}},
  author = {Sonka, Milan and Hlavac, Vaclav and Boyle, Roger},
  year = {2013},
  month = nov,
  publisher = {Springer},
  abstract = {Image Processing, Analysis and Machine Vision represent an exciting part of modern cognitive and computer science. Following an explosion of inter est during the Seventies, the Eighties were characterized by the maturing of the field and the significant growth of active applications; Remote Sensing, Technical Diagnostics, Autonomous Vehicle Guidance and Medical Imaging are the most rapidly developing areas. This progress can be seen in an in creasing number of software and hardware products on the market as well as in a number of digital image processing and machine vision courses offered at universities world-wide. There are many texts available in the areas we cover - most (indeed, all of which we know) are referenced somewhere in this book. The subject suffers, however, from a shortage of texts at the 'elementary' level - that appropriate for undergraduates beginning or completing their studies of the topic, or for Master's students - and the very rapid developments that have taken and are still taking place, which quickly age some of the very good text books produced over the last decade or so. This book reflects the authors' experience in teaching one and two semester undergraduate and graduate courses in Digital Image Processing, Digital Image Analysis, Machine Vision, Pattern Recognition and Intelligent Robotics at their respective institutions.},
  googlebooks = {yA7yBwAAQBAJ},
  isbn = {978-1-4899-3216-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Information Technology}
}

@book{szeliskiComputerVisionAlgorithms2022,
  title = {Computer Vision: Algorithms and Applications},
  shorttitle = {Computer Vision},
  author = {Szeliski, Richard},
  year = {2022},
  publisher = {Springer Nature},
  url = {https://books.google.es/books?hl=es&lr=&id=QptXEAAAQBAJ&oi=fnd&pg=PR9&dq=computer+vision+algorithms+and+applications+2nd+ed&ots=BNvitZVCsk&sig=ecxTFEpxbEYtL2yzTq0MHzbPPpI},
  urldate = {2024-06-09}
}

@misc{ultralyticsConfiguration,
  title = {Configuration},
  author = {Ultralytics},
  url = {https://docs.ultralytics.com/usage/cfg},
  urldate = {2024-06-26},
  abstract = {Optimize your YOLO model's performance with the right settings and hyperparameters. Learn about training, validation, and prediction configurations.},
  langid = {english}
}

@misc{ultralyticsYOLOv8,
  title = {{YOLOv8}},
  author = {Ultralytics},
  url = {https://docs.ultralytics.com/es/models/yolov8},
  urldate = {2024-06-13},
  abstract = {Discover YOLOv8, the latest advancement in real-time object detection, optimizing performance with an array of pre-trained models for diverse tasks.},
  langid = {spanish}
}

@inproceedings{ulucanLargeScaleDatasetFish2020,
  title = {A {{Large-Scale Dataset}} for {{Fish Segmentation}} and {{Classification}}},
  booktitle = {2020 {{Innovations}} in {{Intelligent Systems}} and {{Applications Conference}} ({{ASYU}})},
  author = {Ulucan, Oguzhan and Karakaya, Diclehan and Turkan, Mehmet},
  year = {2020},
  month = oct,
  pages = {1--5},
  doi = {10.1109/ASYU50717.2020.9259867},
  url = {https://ieeexplore.ieee.org/document/9259867},
  urldate = {2024-03-06},
  abstract = {Assessing the quality of seafood both in retail and during packaging at the production side must be carried out minutely in order to avoid spoilage which causes severe human health problems and also economic loss. Since the illnesses and decay in seafood presents distinct symptoms in different species, primarily the classification of species is required. In this field, the inadequacy of the current laborious and slow traditional methods can be overcome with systems based on machine learning and image processing, which present fast and precise results. In order design such systems, practical and suitable datasets are required. However, most of the publicly available datasets are not fit for the mentioned purpose. These datasets either contain images taken underwater or consist of seafood which is generally not (widely) consumed. In this study, a practical and large dataset containing nine distinct seafood widely consumed in the Aegean Region of Turkey is formed. Additionally, comprehensive experiments based on different classification approaches are performed to analyze the usability of this collected dataset. Experimental results demonstrate very promising outcomes; therefore, this dataset will be made publicly available for further investigations in this research domain.},
  keywords = {classification,Economics,feature extraction,Feature extraction,Fish dataset,food quality assessment,Image color analysis,Image segmentation,Quality assessment,segmentation,Task analysis,Training}
}

@inproceedings{xiaSituSeaCucumber2018,
  title = {In {{Situ Sea Cucumber Detection Based}} on {{Deep Learning Approach}}},
  booktitle = {2018 {{OCEANS}} - {{MTS}}/{{IEEE Kobe Techno-Oceans}} ({{OTO}})},
  author = {Xia, Chunlei and Fu, Longwen and Liu, Hui and Chen, Lingxin},
  year = {2018},
  month = may,
  pages = {1--4},
  doi = {10.1109/OCEANSKOBE.2018.8559317},
  url = {https://ieeexplore.ieee.org/document/8559317},
  urldate = {2024-06-13},
  abstract = {Aquaculture provides abundant food that directly consumed by humans. However, aquaculture cultivation is mainly replies on manual management. In order to improve the production efficiency and reduce the labor costs, numbers of recently developed techniques are applied to promote the automation level in aquaculture. In this work, we initiate the study of in situ marine organisms monitoring. Sea cucumber was chosen as the detection target in this paper because the cultivation of sea cucumber has grown quickly in Asian countries. A sea cucumber detection scheme was implemented by utilizing the recently developed YOLO v2 model. Sea cucumber detection model was trained on images selected from one video clip. Experiments were conducted on the other frames from the same video clip. The detection accuracy of 97.6\% was achieved. In addition, sea cucumber detection was tested on images collected from the internet. The detection accuracy on new data was 76.3\%. The detection ability on new targets was remarkable by the deep learning approach. Individual sea cucumbers presenting in various poses or partially occluded were accurately detected in natural scene.},
  keywords = {Aquaculture,Computational modeling,Deep learning,Image color analysis,Lighting,marine organism recognition,marine survey,Object detection,Training,YOLO}
}

@misc{YOLOv5VsYOLOv62022,
  title = {{{YOLOv5}} vs {{YOLOv6}} vs {{YOLOv7}}: {{Comparison}} of {{YOLO Models}} on {{Speed}} and {{Accuracy}} {\textbar} {{CPU}} \& {{GPU}}},
  shorttitle = {{{YOLOv5}} vs {{YOLOv6}} vs {{YOLOv7}}},
  year = {2022},
  month = nov,
  url = {https://learnopencv.com/performance-comparison-of-yolo-models/},
  urldate = {2024-06-05},
  abstract = {We present a etailed Comparison of YOLO Models. Which YOLO model is the fastest? What about inference speed on CPU vs GPU? Which YOLO model is the most accurate?},
  langid = {american}
}

@article{zouObjectDetection202023,
  title = {Object Detection in 20 Years: {{A}} Survey},
  shorttitle = {Object Detection in 20 Years},
  author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  year = {2023},
  journal = {Proceedings of the IEEE},
  volume = {111},
  number = {3},
  pages = {257--276},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10028728/},
  urldate = {2024-06-09}
}

@inproceedings{zouReviewObjectDetection2019,
  title = {A {{Review}} of {{Object Detection Techniques}}},
  booktitle = {2019 {{International Conference}} on {{Smart Grid}} and {{Electrical Automation}} ({{ICSGEA}})},
  author = {Zou, Xinrui},
  year = {2019},
  month = aug,
  pages = {251--254},
  doi = {10.1109/ICSGEA.2019.00065},
  url = {https://ieeexplore.ieee.org/abstract/document/8901325},
  urldate = {2024-02-07},
  abstract = {Object detection is widely used in the field of computer vision and crucial for variety of applications, e.g., self-driving car. During the development of half a century, object detection methods have been continuously developed, and generated numerous approaches which obtained promising achievements. At present, the approach of object detection has been largely evolved into two categories which are traditional machine learning methods utilizing varied computer vision techniques and deep learning method. This article presents a review of object detection techniques. Firstly, the existing methods based on traditional machine learning are summarized and introduced. Then, two main schools of deep learning methods, R-CNN and YOLO, are selected for analysis and introduction. At the end of the article, the methods mentioned are briefly compared and discussed.},
  keywords = {Computer Vision,Machine Learning,Target Detection}
}
